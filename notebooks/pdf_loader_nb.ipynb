{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344420fc-8bef-41cf-a396-43a1527fe2e2",
   "metadata": {},
   "source": [
    "## RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cb0d8-bb67-4bb5-abc0-e316b18d280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the necessary libraries\n",
    "from msilib.schema import Class\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a518162-3f73-481b-bad6-cfb7f815e75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\Projects\\GitHub Repository_AI-Projects_Destination\\AI-Projects\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98ed148-db01-494e-9b7a-b3ffb3be78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files.\n",
      "Processing file: ..\\data\\pdf\\aao5646-kroodsma-sm.pdf\n",
      "Loaded 40 pages from ..\\data\\pdf\\aao5646-kroodsma-sm.pdf\n",
      "Processing file: ..\\data\\pdf\\AI_ML_DS_Interview_Roadmap.pdf\n",
      "Loaded 2 pages from ..\\data\\pdf\\AI_ML_DS_Interview_Roadmap.pdf\n",
      "\n",
      "Total documents loaded: 42\n"
     ]
    }
   ],
   "source": [
    "# Read all the files inside the directory and sub-directories   \n",
    "def process_all_pdfs(directory_path):\n",
    "    \"\"\"Process all PDF files in the given directory and return a list of documents.\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_files = list(Path(directory_path).rglob(\"**/*.pdf\"))  # Recursively find all PDF files\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "#Batch PDF Document Loading and Metadata Enrichment using PyMuPDFLoader\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file}\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            #Add source information also to the document metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source\"] = str(pdf_file)\n",
    "                doc.metadata[\"file_type\"] = \"pdf\"\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages from {pdf_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Following error occurred while loading {pdf_file} with PyMuPDFLoader: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf/\")\n",
    "#print(all_pdf_documents) # Print the first document to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8dc709f-0502-4748-88d5-b10aa2d4c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 42 documents into 112 chunks.\n",
      "\n",
      "First Chunk as Example Chunk:\n",
      "\n",
      "First chunk preview: www.sciencemag.org/content/359/6378/904/suppl/DC1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Supplementary Materials for \n",
      " \n",
      "Tracking the global footprint of fisheries \n",
      " \n",
      "David A. Kroodsma,* Juan Mayorga, Timothy Hochberg, Nathan...\n",
      "First chunk metadata: {'producer': 'Adobe PDF Library 11.0', 'creator': 'Acrobat PDFMaker 11 for Word', 'creationdate': '2018-02-14T15:19:51-05:00', 'source': '..\\\\data\\\\pdf\\\\aao5646-kroodsma-sm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\aao5646-kroodsma-sm.pdf', 'total_pages': 40, 'format': 'PDF 1.6', 'title': '', 'author': 'David Kroodsma', 'subject': '', 'keywords': '', 'moddate': '2018-02-14T15:24:29-05:00', 'trapped': '', 'modDate': \"D:20180214152429-05'00'\", 'creationDate': \"D:20180214151951-05'00'\", 'page': 0, 'file_type': 'pdf'}\n",
      "First Chunk Content: www.sciencemag.org/content/359/6378/904/suppl/DC1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Supplementary Materials for \n",
      " \n",
      "Tracking the global footprint of fisheries \n",
      " \n",
      "David A. Kroodsma,* Juan Mayorga, Timothy Hochberg, Nathan A. Miller,  \n",
      "Kristina Boerder, Francesco Ferretti, Alex Wilson, Bjorn Bergman, Timothy D. White, \n",
      "Barbara A. Block, Paul Woods, Brian Sullivan, Christopher Costello, Boris Worm \n",
      " \n",
      "*Corresponding author. Email: david@globalfishingwatch.org \n",
      " \n",
      "Published 23 February 2018, Science 359, 904 (2018) \n",
      "DOI: 10.1126/science.aao5646 \n",
      " \n",
      "This PDF file includes: \n",
      " \n",
      "Materials and Methods \n",
      "Figs. S1 to S9 \n",
      "Tables S1 to S8 \n",
      "References\n",
      "page_content='www.sciencemag.org/content/359/6378/904/suppl/DC1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Supplementary Materials for \n",
      " \n",
      "Tracking the global footprint of fisheries \n",
      " \n",
      "David A. Kroodsma,* Juan Mayorga, Timothy Hochberg, Nathan A. Miller,  \n",
      "Kristina Boerder, Francesco Ferretti, Alex Wilson, Bjorn Bergman, Timothy D. White, \n",
      "Barbara A. Block, Paul Woods, Brian Sullivan, Christopher Costello, Boris Worm \n",
      " \n",
      "*Corresponding author. Email: david@globalfishingwatch.org \n",
      " \n",
      "Published 23 February 2018, Science 359, 904 (2018) \n",
      "DOI: 10.1126/science.aao5646 \n",
      " \n",
      "This PDF file includes: \n",
      " \n",
      "Materials and Methods \n",
      "Figs. S1 to S9 \n",
      "Tables S1 to S8 \n",
      "References' metadata={'producer': 'Adobe PDF Library 11.0', 'creator': 'Acrobat PDFMaker 11 for Word', 'creationdate': '2018-02-14T15:19:51-05:00', 'source': '..\\\\data\\\\pdf\\\\aao5646-kroodsma-sm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\aao5646-kroodsma-sm.pdf', 'total_pages': 40, 'format': 'PDF 1.6', 'title': '', 'author': 'David Kroodsma', 'subject': '', 'keywords': '', 'moddate': '2018-02-14T15:24:29-05:00', 'trapped': '', 'modDate': \"D:20180214152429-05'00'\", 'creationDate': \"D:20180214151951-05'00'\", 'page': 0, 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Text Splitting documents into chunks\n",
    "\n",
    "def text_splitting(documents, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"Split documents into smaller chunk for better RAG Performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    try:\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        print(f\"Split {len(documents)} documents into {len(splits)} chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting document {documents.metadata.get('source', 'unknown')}: {e}\")\n",
    "\n",
    "    if splits:\n",
    "        print(f\"\\nFirst Chunk as Example Chunk:\")\n",
    "        print(f\"\\nFirst chunk preview: {splits[0].page_content[:200]}...\")  # Print first 200 characters of the first chunk\n",
    "        print(f\"First chunk metadata: {splits[0].metadata}\")\n",
    "        print(f\"First Chunk Content: {splits[0].page_content}\")\n",
    "    return splits\n",
    "\n",
    "split_chunks = text_splitting(all_pdf_documents, chunk_size=1000, chunk_overlap=100)\n",
    "print(split_chunks[0]) # Print the first chunk to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdf7650-f4fd-4c88-b3a9-92c4a9540982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-MiniLM-L6-v2\n",
      "Loaded embedding model: all-MiniLM-L6-v2 successfully. Embedding Dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x20f490c5970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding & VectorStore DB\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer ## Embedding Model\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"The class manages embedding generation using a SentenceTransformer model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the Sentence Transformer Embedding model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name for sentence embeddings.\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.model_name = model_name\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the Sentence Transformer embedding model. \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Loaded embedding model: {self.model_name} successfully. Embedding Dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embedding(self, chunk_text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for list of chunk texts.\n",
    "\n",
    "        Args:\n",
    "            chunk_text : list of texts to generate embeddings for.\n",
    "\n",
    "        Returns :\n",
    "        numpy array of embeddings with shape (len(chunk_text), embedding_dimension)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(chunk_text)} number of chunk texts...\")\n",
    "        #embeddings = self.model.encode(chunk_text, show_progress_bar=True, batch_size=16, convert_to_numpy=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        embeddings = self.model.encode(chunk_text, show_progress_bar=True)\n",
    "\n",
    "        print(f\"embeddings created\")\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    def embedding_dimension(self) -> int:\n",
    "        # Return the dimension of the embeddings\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        return self.model.get_sentence_embedding_dimension() # Not necessary to have a separate function created to check the embedding dimension and print it out. As you can see its already included under _load_model() function.\n",
    "    \"\"\"\n",
    "\n",
    "# Initialize Embedding Manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6185d721-288a-4863-9c5c-f427d1bf662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "✅ Initialized ChromaDB vector store: pdf_documents\n",
      "📦 Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStoreManager at 0x20f4930e780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VECTOR STORE - ChromaDB\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manages ChromaDB vector store operations including adding, querying, and deleting documents.\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/VectorStore/chroma_db\"):\n",
    "        \"\"\"\n",
    "        Initialize ChromaDB client and collection.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): Name of the ChromaDB collection.\n",
    "            persist_directory (str): Directory to persist the vector store or ChromaDB data.\n",
    "        \"\"\"\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        print(1)\n",
    "        self._initialize_store()  # Connect to or create the index / storage layer where your RAG data lives.\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB Client and Collection.\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB Client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)  # Create directory if it doesn't exist\n",
    "            print(2)\n",
    "\n",
    "            try:\n",
    "                self.client = chromadb.PersistentClient(path=self.persist_directory,)\n",
    "                print(3)\n",
    "            except Exception as e:\n",
    "                print(f\" Error creating Chromadb Client :{e}\")\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF Documents Embeddings For RAG\"}\n",
    "            )\n",
    "            print(4)\n",
    "\n",
    "            print(f\"✅ Initialized ChromaDB vector store: {self.collection_name}\")\n",
    "            print(f\"📦 Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error initializing ChromaDB vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, chunks: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add chunks document and embeddings to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[Any]): List of chunks with 'page_content' and 'metadata'.\n",
    "            embeddings (np.ndarray): Corresponding embeddings for list of LangChain chunks.\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            print(\"⚠️ No chunks to add as documents.\")\n",
    "            return\n",
    "        if len(chunks) != len(embeddings):\n",
    "            raise ValueError(\"Number of chunks and embeddings must match.\")\n",
    "        \n",
    "        print(f\"🧩 Adding {len(chunks)} chunks as documents to the vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        chunks_text_doc = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        print(3)\n",
    "\n",
    "        for i, (doc, emb) in enumerate(zip(chunks, embeddings)):\n",
    "            print(4)\n",
    "            doc_id = str(uuid.uuid4())  # Generate a unique ID for each chunk document. # Generate a unique ID for each chunk document # can try out another unique ID creation as per Krish Naik's RG LangChain Video also required\n",
    "            ids.append(doc_id)\n",
    "            print(5)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata) if hasattr(doc, \"metadata\") else {}\n",
    "            print(6)\n",
    "\n",
    "            metadata[\"chunk_index\"] = i\n",
    "            metadata[\"content_length\"] = len(getattr(doc, \"page_content\", \"\"))\n",
    "            print(7)\n",
    "\n",
    "            metadatas.append(metadata)\n",
    "            print(8)\n",
    "\n",
    "\n",
    "            # Prepare document text\n",
    "            chunks_text_doc.append(getattr(doc, \"page_content\", \"\"))\n",
    "            print(9)\n",
    "\n",
    "\n",
    "            # Prepare embeddings\n",
    "            embeddings_list.append(emb.tolist())  # Convert numpy array to list\n",
    "            print(10)\n",
    "            \n",
    "            print (f\"count: {i}\")\n",
    "\n",
    "        \n",
    "        # Add to ChromaDB collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=chunks_text_doc\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Added {len(documents)} chunk documents to the vector store.\")\n",
    "            print(f\"📈 Total chunk documents in collection now: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding chunk documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "# Instantiate and verify initialization\n",
    "vector_store_manager = VectorStoreManager()\n",
    "vector_store_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff4848-9e6f-43c4-89ea-6f7e8438bd14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "998\n",
      "Generating embeddings for 5 number of chunk texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c683c26fae4a95b2335c85c2533243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Text to Embeddings\n",
    "\n",
    "chunk_text = [doc.page_content for doc in split_chunks]\n",
    "print(1)\n",
    "print(f\"{max(len(t) for t in chunk_text)}\")\n",
    "embeddings = embedding_manager.generate_embedding(chunk_text[:5])\n",
    "print(2)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Store the embeddings in the chromadb collections\n",
    "vector_store_manager.add_documents(split_chunks[:5], embeddings)\n",
    "print(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad8de2-8426-4337-ae07-604c69235fea",
   "metadata": {},
   "source": [
    "## RAG Pipelines - Retriever Pipeline From Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f7adb-2810-4691-b4c2-3938818324fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class RAGRetriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
